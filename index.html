<!DOCTYPE html>
<html lang="en">
<head>
<br/>
<br/>
<title>Shiry Ginosar - Homepage</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0" charset="utf-8">
<!-- jQuery -->
<script src="http://code.jquery.com/jquery.min.js"></script>
<!-- Bootstrap -->
<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
<!-- Bootstrap -->
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
<link rel="stylesheet" type="text/css" href="styles/main.css"/>
</head>
<body>

<div class="container">
<div class="row">
	<div class="col-md-3 left-column">
		<img src="images/me_2023_small.png" alt="me" height="200" class="img-rounded">
		<!-- <h2>Contact Info</h2> -->

		<!-- <address>
  			<strong>Mailing Address</strong><br>
  			UCB EECS, MC 1776<br/>
			387 Soda Hall<br/>
			Berkeley, CA 94720-1776<br/>
			United States<br/>
		</address> -->

		<address>
			<br>
  			<!-- <strong>Email</strong><br> -->
  			<!-- shiry <span class="grey">at</span> eecs <span class="grey">dot</span> berkeley <span class="grey">dot</span> edu -->
  			shiry <span class="grey">at</span> ttic <span class="grey">dot</span> edu
        <!-- <br>
        shiry <span class="grey">at</span> google <span class="grey">dot</span> com -->
  			<br>
  			<a href="https://scholar.google.com/citations?user=bQNISbAAAAAJ&hl=en">Google Scholar</a> /
  			<a href="https://twitter.com/shiryginosar">Twitter</a>
		</address>
	</div>

	<div class="col-md-7">
		<div class="page-header">
			<h1>Shiry Ginosar</h1>
		</div>

		<p>
			I am an Assistant Professor at <a href="https://www.ttic.edu/">TTIC</a> and a Visiting Faculty Researcher at <a href="https://deepmind.google/">Google DeepMind</a>.
		</p>

		<p>
			Previously, I was a postdoctoral fellow at the <a href="https://simons.berkeley.edu/">Simons Institute for the Theory of Computing</a> and a <a href="https://cifellows2020.org">Computing Innovation postdoctoral fellow</a> at <a href="http://www.eecs.berkeley.edu">UC Berkeley</a>, advised by <a href="http://www.eecs.berkeley.edu/~malik/">Jitendra Malik</a>.
      I completed my Ph.D. in Computer Science at <a href="http://www.berkeley.edu">UC Berkeley</a> under the supervision of <a href="http://www.eecs.berkeley.edu/~efros/">Alyosha Efros</a>. Before joining the Computer Vision group, I was part of <a href="https://people.eecs.berkeley.edu/~bjoern/">Bjoern Hartmann</a>'s Human-Computer Interaction lab at Berkeley. Earlier in my career, I was a Visiting Scholar at the <a href="http://www.cs.cmu.edu">CS Department</a> of <a href="http://www.cmu.edu">Carnegie Mellon University</a> with <a href="http://www.cs.cmu.edu/~biglou">Luis von Ahn</a> and <a href="https://www.cs.cmu.edu/~mblum/">Manuel Blum</a> in <a href="http://en.wikipedia.org/wiki/Human-based_computation">Human Computation</a>. Between my academic roles, I spent four years at <a href="http://www.endeca.com">Endeca</a> as a Senior Software Engineer. In the distant past, I trained fighter pilots in <a href="https://en.wikipedia.org/wiki/McDonnell_Douglas_F-4_Phantom_II"> F-4 Phantom</a> flight simulators as a Staff Sergeant in the Israeli Air Force.
		</p>
		<p>
			My research has been covered by <a href="https://www.newyorker.com/magazine/2018/11/12/in-the-age-of-ai-is-seeing-still-believing">The New Yorker</a>, <a href="https://www.wsj.com/articles/deepfake-videos-are-ruining-lives-is-democracy-next-1539595787">The Wall Street Journal</a>, and the <a href="https://www.washingtonpost.com/news/wonk/wp/2015/12/01/researchers-have-discovered-a-surprising-reason-we-smile-in-photos/?noredirect=on&utm_term=.5e52a9d6fa4b">Washington Post</a>, amongst others. My work has been featured on <a href="https://www.pbs.org/video/deepfake-videos-are-getting-terrifyingly-real-xywbdx/">PBS NOVA</a>, exhibited at the <a href="http://www.dmh.org.il/">Israeli Design Museum</a> and is part of the permanent collection of the <a href="https://www.deutsches-museum.de/">Deutsches Museum</a>. My patent-pending research work inspired the founding of Sway, a startup acquired by <a href="https://corp.roblox.com/">Roblox</a>. I have been named a <a href="https://publish.illinois.edu/rising-stars/participants/">Rising Star in EECS</a>, and am a recipient of the U.S. National Science Foundation <a href="http://www.nsfgrfp.org/">Graduate Research Fellowship</a>, the California Legislature Grant for graduate studies, and the Samuel Silver Memorial Scholarship Award for combining intellectual achievement in science and engineering with serious humanistic and cultural interests.
		</p>
		<br>

    <h2>News</h2>
    <p>
    I joined <a href="https://www.ttic.edu/">TTIC</a> as an assistant professor!
    </p>
    <p>
    I recently organized a <a href="https://simons.berkeley.edu/programs/summer-cluster-ai-psychology-neuroscience">Summer Cluster</a> at the <a href="https://simons.berkeley.edu">Simons Institute for the Theory of Computing</a> at UC Berkeley on studying intelligence from a fundamental scientific viewpoint, bringing together top AI, Psychology, and Neuroscience researchers. Talk recordings:
      <ul>
        <li><a href="https://simons.berkeley.edu/workshops/understanding-lower-level-intelligence-ai-psychology-neuroscience-perspectives/videos#simons-tabs#simons-tabs">workshop 1 on lower-level capabilities</a></li>
        <li><a href="https://simons.berkeley.edu/workshops/understanding-higher-level-intelligence-ai-psychology-neuroscience-perspectives/videos#simons-tabs">workshop 2 on higher-level capabilities</a></li>
      </ul>
    </p>
    <br>

    <h2>Recent Talks</h2>
    <p>
      <ul>
        <li><a href="https://simons.berkeley.edu/talks/shiry-ginosar-uc-berkeley-2024-06-03">What is the right "token" for next-token visual prediction?</a></li>
        <li><a href="https://www.youtube.com/watch?v=pSlDAq_zlBQ&list=PLgKuh-lKre10bhuVk0OV2gb8uoCItOuP9&index=21">Social behavior prediction from video observations.</a></li>
      </ul>
      Both at the <a href="https://simons.berkeley.edu">Simons Institute for the Theory of Computing</a>, June 2024. 
    </p>
    <br>

		<h2>Selected Publications</h2>
    <h5>See <a href="https://scholar.google.com/citations?user=bQNISbAAAAAJ&hl=en">Google Scholar</a> for the full list.</h5>
    <br>

		<ul class="media-list, citations">

		<!--
  		<li class="media">
    		<a class="pull-left" href="#">
      		<img class="media-object img-responsive" src="..." alt="..." width="200">
    		</a>
    		<div class="media-body">
      		<h4 class="media-heading">Media heading
          <p><small>
            ...
          </p></small>
          </h4>
      		...
    		</div>
  		</li>
		-->
    <li class="media">
        <a class="pull-left" href="#">
          <img class="media-object img-responsive" src="images/SwingDance.gif" alt="swing dance motion prediction" width="200">
        </a>
        <div class="media-body">
          <h4 class="media-heading">Synergy and Synchrony in Couple Dances
          <p>
            <small>
              <em>
                How does social interaction influence one's behavior?
              </em>
              <br>  
               We focus our analysis on motion behavior during Swing, a couples-dance genre with tight physical coupling, for which we present an in-the-wild video dataset. We demonstrate that single-person future motion prediction in this context is challenging. Instead, we observe that prediction greatly benefits from considering the interaction partners' behavior.
            </small>
          </p>
          </h4>
          <p>
           Vongani Maluleke, Lea Müller, Jathushan Rajasegaran, Georgios Pavlakos, <strong>Shiry Ginosar</strong>, Angjoo Kanazawa, and Jitendra Malik. <span class="cite-title">Synergy and Synchrony in Couple Dance</span>, In submission. <a href="https://arxiv.org/abs/2409.04440">PDF</a>, <a href="https://von31.github.io/synNsync/">Project Page</a>
          </p>
        </div>
    </li>
    <br>  
    <li class="media">
        <a class="pull-left" href="#">
          <img class="media-object img-responsive" src="images/GMAE_method.png" alt="gaussian masked autoencoders" width="200">
        </a>
        <div class="media-body">
          <h4 class="media-heading">Gaussian Splatting for Masked Autoencoders
          <p>
            <small>
              <em>
                3D Gaussians as a learned mid-level image representation.
              </em>
              <br>  
            We present <strong>the first method to employ 3D Gaussian primitives in an image representation learning framework</strong> trained on large image datasets. Our learning-based method starkly contrasts with the current usage of Gaussian splatting, which is restricted to optimization-based single-scene reconstructions.
            </small>
          </p>
          </h4>
          <p>
           Jathushan Rajasegaran, Xinlei Chen, Ruilong Li, Christoph Feichtenhofer, <strong>Shiry Ginosar</strong>, and Jitendra Malik. <span class="cite-title">Gaussian Splatting for Masked Autoencoders</span>, In submission. <a href="https://simons.berkeley.edu/talks/shiry-ginosar-uc-berkeley-2024-06-03">The talk I gave at the Simons Institute at Berkeley.</a> 
          </p>
        </div>
    </li>
    <br>  
    <li class="media">
        <a class="pull-left" href="#">
          <img class="media-object img-responsive" src="images/KiVA.jpg" alt="a visual analogies benchmark" width="200">
        </a>
        <div class="media-body">
          <h4 class="media-heading">KiVA: Kid-inspired Visual Analogies for LMMs
          <p>
          <small>
            <em>
              A benchmark for visual analogies inspired by developmental psychology.
            </em>
            <br>
            We present a benchmark that closes a critical gap in current benchmarks for foundational models - visual analogical reasoning, which even young children can do but in which large multimodal models perform poorly.
          </small>
          </p>
          </h4>
          <p>
           Eunice Yiu, Maan Qraitem, Anisa Noor Majhi, Charlie Rose Wong, Yutong Bai, <strong>Shiry Ginosar</strong>, Alison Gopnik, and Kate Saenko. <span class="cite-title">KiVA: Kid-inspired Visual Analogies for Large Multimodal Models</span>, In submission. <a href="https://arxiv.org/abs/2407.17773">PDF</a>, <a href="https://github.com/ey242/KiVA">Benchmark</a>
          </p>
        </div>
    </li>
    <br>  
    <li class="media">
        <a class="pull-left" href="https://diff-mining.github.io/">
          <img class="media-object img-responsive" src="images/diffusion_mining.jpg" alt="coming soon" width="200">
        </a>
        <div class="media-body">
          <h4 class="media-heading">Diffusion Models as Data Mining Tools
          <p>
          <small>
            <em>
              Image synthesis models can be used for visual data mining! 
            </em>
            <br>
            <strong>Central insight:</strong> Since generative models learn an accurate representation of their training data, we can use them to summarize the data by mining for visual patterns.
            <br>
            Our analysis-by-synthesis approach to data mining has two key advantages. First, it scales much better than traditional correspondence-based approaches, as it does not require explicitly comparing all pairs of visual elements. Second, while most previous works on visual data mining focus on a single dataset, our approach works on diverse datasets in terms of content and scale. Furthermore, our approach allows for translating visual elements across class labels and analyzing consistent changes, opening up new possibilities for data mining in the future.
          </small>
          </p>
          </h4>
          <p>
          Ioannis Siglidis, Aleksander Holynski, Alexei A. Efros, Mathieu Aubry, and <strong>Shiry Ginosar</strong>. <span class="cite-title">Diffusion Models as Data Mining Tools</span>, ECCV 2024. <a href="https://www.arxiv.org/abs/2408.02752">PDF</a>, <a href="https://diff-mining.github.io/">Project Page</a>
          </p>
        </div>
    </li>
    <br>  
    <li class="media">
        <a class="pull-left" href="https://prosepose.github.io/">
          <img class="media-object img-responsive" src="images/prosepose.png" alt="ProsePose improves pose estimates involving physical contact using LMMs" width="200">
        </a>
        <div class="media-body">
          <h4 class="media-heading">Pose Priors from Language Models
          <p>
         <small>
          <em>
            Improving pose estimates involving physical contact using LMMs.
          </em>
          <br>
          <strong>Central insight:</strong> An LLM can tell you, e.g., that to hug someone, your arm should go on their neck, shoulder, waist, or bottom—but probably not on their knee... And all of this knowledge comes from detailed descriptions of physical contact in books!
          <br>
          We leverage this insight to improve pose estimation by converting natural language descriptors, generated by a large multimodal model (LMM), into tractable losses to constrain 3D pose optimization.
        </small>
          </h4>
      </p>
      <p>
      Sanjay Subramanian, Evonne Ng, Lea M&uuml;ller Dan Klein, <strong>Shiry Ginosar</strong>, and Trevor Darrell.<span class="cite-title"> Pose Priors from Language Models</span>, In submission. 
      <a href="https://arxiv.org/abs/2405.03689">PDF</a>, <a href="https://prosepose.github.io/">Project Page</a>
      </div>
      </li>
    <br>
  <li class="media">
        <a class="pull-left" href="https://people.eecs.berkeley.edu/~evonne_ng/projects/text2listen/">
          <img class="media-object img-responsive" src="images/text2listen.gif" alt="predicting listener response from speaker text" width="200">
        </a>
        <div class="media-body">
          <h4 class="media-heading">Can Language Models Learn to Listen?
          <p>
         <small>
          <em>
            Squeezing lexical semantic "juice" out of large language models.
          </em>
          <br>
          Given only the spoken text of a speaker, we synthesize a realistic, synchronous listener. Our text-based model responds in an emotionally-appropriate manner when lexical semantics is crucial. For example, when it is not appropriate to smile despite a speaker's uneasy laughter. Technically, we squeeze out as much semantic "juice" as possible from a pretrained large language model by finetuning it to autoregressively generate realistic 3D listener motion in response to the input transcript.
          <br>
          <strong>Main innovation:</strong> We treat atomic gesture elements as novel language tokens easily ingestible by language models. We can then finetune LLMs to synthesize motion by predicting sequences of these elements.
        </small>
          </h4>
      </p>
      <p>
      Evonne Ng*, Sanjay Subramanian*, Dan Klein, Angjoo Kanazawa, Trevor Darrell, and <strong>Shiry Ginosar</strong>.<span class="cite-title"> Can Language Models Learn to Listen?</span>, ICCV 2023. 
      <a href="https://arxiv.org/abs/2308.10897">PDF</a>,
<a href="https://www.youtube.com/watch?v=djpSOhdIU8M">Video</a>, <a href="https://people.eecs.berkeley.edu/~evonne_ng/projects/text2listen/">Project Page</a>
      </div>
      </li>
    
    <br>
      <li class="media">
        <a class="pull-left" href="https://imagine.enpc.fr/~shenx/HisImgAnalysis/">
          <img class="media-object img-responsive" src="images/thumbnail-2022-history.jpg" alt="pattern detection in artworks" width="200">
        </a>
        <div class="media-body">
          <h4 class="media-heading">Historical image analysis
          <p><small>
            <em>
              A unified approach to historical watermark recognition, and repeated-pattern detection in artwork collections.
            </em>
            <br>
            We explore two applications of computer vision to analyze historical data: watermark recognition and one-shot repeated pattern detection in artwork collections. Both problems present computer vision challenges, which are representative of the ones encountered in cultural heritage applications: limited supervision is available, the tasks are fine-grained recognition, and the data comes in several different modalities. We use a single method for both tasks: we define an image similarity score based on geometric verification of mid-level features and show how spatial consistency can be used to fine-tune out-of-the-box features for the target dataset with weak or no supervision.
          </small>
          </p>
          </h4>
          <p>
           Xi Shen, Robin Champenois, <strong>Shiry Ginosar</strong>,  Ilaria Pastrolin,   Morgane Rousselot, Oumayma Bounou, Tom Monnier, Spyros Gidaris, François Bougard,  Pierre-Guillaume Raverdy, Marie-Françoise Limon, Christine Bénévent, Marc Smith, Olivier Poncet, K. Bender, Béatrice Joyeux-Prunel, Elizabeth Honig, Alexei A. Efros,  and Mathieu Aubry. <span class="cite-title">Spatially-consistent Feature Matching and Learning for Heritage Image Analysis</span>, IJCV 2022. <a href="https://hal.science/hal-03620996">PDF</a>,
          <a href="https://imagine.enpc.fr/~shenx/HisImgAnalysis/">Project Page</a>
          </p>
        </div>
    </li>
    <br>  

	<li class="media">
        <a class="pull-left" href="https://evonneng.github.io/learning2listen/">
          <img class="media-object img-responsive" src="images/learning2listen.gif" alt="predicting listener response from speaker speech" width="200">
        </a>
        <div class="media-body">
          <h4 class="media-heading">Learning2Listen
          <p>
         <small>
        <em>
          Learning to respond like a good listener.
        </em>
          <br>
          Given a speaker, we synthesize a realistic, synchronous listener. To do this, we learn human interaction 101: the delicate dance of non-verbal communication. We expect good listeners to look us in the eye, synchronize their motion with ours, and mirror our emotions. You can't annotate this! So we must learn from raw data. Technically, we are the first to extend vector-quantization methods to motion synthesis. We show that our novel sequence-encoding VQ-VAE, coupled with a transformer-based prediction mechanism, performs much better than competitive methods for motion generation.
      	</small>
          </h4>
      </p>
      <p>
      Evonne Ng, Hanbyul Joo, Liwen Hu, Hao Li, Trevor Darrell, Angjoo Kanazawa, and <strong>Shiry Ginosar</strong>.<span class="cite-title"> Learning to Listen:
Modeling Non-Deterministic Dyadic Facial Motion</span>, CVPR 2022. 
      <a href="https://arxiv.org/abs/2204.08451">PDF</a>,
<a href="https://www.youtube.com/watch?v=3CSlKZ5T6DU">Video</a>, <a href="https://evonneng.github.io/learning2listen/">Project Page</a>
      </div>
      </li>
    <br>
    <li class="media">
        <a class="pull-left" href="https://medhini.github.io/audio_video_textures/">
          <img class="media-object img-responsive" src="images/strumming.jpg" alt="man strumming on guitar" width="200">
        </a>
        <div class="media-body">
          <h4 class="media-heading">Audio-Conditioned Contrastive Video Textures
          <p>
            <small>
              <em>
                Learning to synthesize video textures conditioned on audio
              </em>
              <br>
            We learn a representation for creating video textures using contrastive learning between the frames of a video and the associated audio.
            </small>
          </p>
          </h4>
          <p>
           Medhini Narasimhan, <strong>Shiry Ginosar</strong>, Andrew Owens, Alexei A. Efros, and Trevor Darrell. <span class="cite-title">Strumming to the Beat: Audio-Conditioned Contrastive Video Textures</span>, WACV 2022. <strong>Best Paper Honorable Mention</strong> <strong>(Oral)</strong>. <a href="https://arxiv.org/pdf/2104.02687">PDF</a>,
          <a href="https://medhini.github.io/audio_video_textures/">Project Page</a>
          </p>
        </div>
    </li>
    <br>  
 	<li class="media">
        <a class="pull-left" href="http://people.eecs.berkeley.edu/~evonne_ng/projects/body2hands/">
          <img class="media-object img-responsive" src="images/body2hands.gif" alt="predicting hand shape from body motion" width="200">
        </a>
        <div class="media-body">
          <h4 class="media-heading">Body2Hands
          <p>
         <small>
        <em>
          Learning to Infer 3D Hands from Conversational Gesture Body Dynamics.
        </em>
          <br>
          A novel learned deep prior of body motion for 3D hand shape synthesis and estimation in the domain of conversational gestures. Our model builds upon the insight that body motion and hand gestures are strongly correlated in non-verbal communication settings. We formulate the learning of this prior as a prediction task of 3D hand shape given body motion input alone. 
      	</small>
          </h4>
      </p>
      <p>
      Evonne Ng, <strong>Shiry Ginosar</strong>, Trevor Darrell and Hanbyul Joo. <span class="cite-title">Body2Hands: Learning to Infer 3D Hands from Conversational Gesture Body Dynamics</span>, CVPR 2021. 
      <a href="https://arxiv.org/abs/2007.12287">PDF</a>,
<a href="https://youtu.be/9Ohsg3vB7Vs">Video</a>, <a href="http://people.eecs.berkeley.edu/~evonne_ng/projects/body2hands/">Project Page</a>
       </div>
      </li>
    <br>


 	<li class="media">
        <a class="pull-left" href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2020/EECS-2020-148.html">
          <img class="media-object img-responsive" src="images/dissertation_committee_square.jpg" alt="dissertation committee on zoom" width="200">
        </a>
        <div class="media-body">
          <h4 class="media-heading">Modeling Visual Minutiae: Gestures, Styles, and Temporal Patterns
          <p><small>
          Ph.D. Dissertation.
        <!--- write something -->
          </small></h4>
            <p>
      <a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2020/EECS-2020-148.html">Dissertation</a>,
      <a href="publications/appendix.pdf">Appendix: Dissertation in the Time of Corona</a>
            </p>
        </div>
      </li>

    <br>
 	<li class="media">
        <a class="pull-left" href="https://factorize-a-city.github.io">
          <img class="media-object img-responsive" src="images/paris_rot_web.gif" alt="modifying sun azimuth" width="200">
        </a>
        <div class="media-body">
          <h4 class="media-heading">Learning to Factorize and Relight a City
          <p>
         <small>
        <em>
          Disentangle changing factors from permanent ones.
        </em>
          <br>
          We disentangle outdoor scenes into temporally-varying illumination and permanent scene factors. To facilitate training, we assemble a city-scale dataset of outdoor timelapse imagery from Google Street View Time Machine, where the same locations are captured repeatedly through time. Our learned disentangled factors can be used to manipulate novel images in realistic ways, such as changing lighting effects and scene geometry.
      	</small>
          </h4>
      </p>
                  <p>
      Andrew Liu, <strong>Shiry Ginosar</strong>, Tinghui Zhou, Alexei A. Efros and Noah Snavely. <span class="cite-title">Learning to Factorize and Relight a City</span>, ECCV 2020. 
      <a href="http://arxiv.org/abs/2008.02796">PDF</a>,
      	<!-- <a href="#plenoptic" role="button" data-toggle="collapse" data-target="#collapse-plenoptic" aria-expanded="false" aria-controls="collapse">BibTeX</a>, --> <a href="https://youtu.be/-0_gmw_oxvA">Video</a>, <a href="https://factorize-a-city.github.io">Project Page</a> 
              <div class="collapse" id="collapse-plenoptic">
              <div class="well">
  @inproceedings{Liu2020city,<br>
  &nbsp;&nbsp;author = {Liu, Andrew and Ginosar, Shiry and Zhou, Tinghui and Snavely, Noah and Efros, Alexei A.},<br>
  &nbsp;&nbsp;title = {Learning to Factorize and Relight a City},<br>
  &nbsp;&nbsp;booktitle = {European Conference on Computer Vision (ECCV)},<br>
  &nbsp;&nbsp;year = 2020,<br>
  &nbsp;&nbsp;}
              </div>
              </div>
            </p>
        </div>
      </li>
    <br>
	<li class="media">
    	<a class="pull-left" href="projects/speech2gesture/index.html">
      		<!-- <img class="media-object img-responsive" src="images/gesture-fig.png" alt="speech to gesture translation" width="200"> -->
          <img class="media-object img-responsive" src="images/gestures_CVPR_2019_demo.gif" alt="speech to gesture translation" width="200">

    	</a>
    	<div class="media-body">
      		<h4 class="media-heading">Learning Individual Styles of Conversational Gesture
          		<p><small>
                <em>
          			Audio to motion translation.
              </em>
          			<br>
          			Human speech is often accompanied by hand and arm gestures.
					Given audio speech input, we generate plausible gestures to go along with the sound.
					Specifically, we perform cross-modal translation from ``in-the-wild'' monologue speech of a
					single speaker to their hand and arm motion. 
					We train on unlabeled videos for which we only have noisy pseudo ground truth from an automatic pose detection system.
					We release a large video dataset of person-specific gestures.
          		</p></small>
          	</h4>
          	<strong>Shiry Ginosar</strong>*, Amir Bar*, Gefen Kohavi, Caroline Chan, Andrew Owens and Jitendra Malik. <span class="cite-title">Learning Individual Styles of Conversational Gesture</span>, CVPR 2019. <a href="https://arxiv.org/abs/1906.04160">PDF</a>,<!--  <a href="#gestures" role="button" data-toggle="collapse" data-target="#collapse-gestures" aria-expanded="false" aria-controls="collapse">BibTeX</a>, <a href="https://youtu.be/xzTE5sobpFY">Video</a>, --> <a href="projects/speech2gesture/index.html">Project Page</a>
          		<div class="collapse" id="collapse-gestures">
              <div class="well">
  @inproceedings{ginosar2019,<br>
  &nbsp;&nbsp;title={Learning Individual Styles of Conversational Gesture},<br>
  &nbsp;&nbsp;author={Ginosar, Shiry and Bar, Amir and Kohavi, Gefen and Chan, Caroline and Owens, Andrew and Malik, Jitendra},<br>
  &nbsp;&nbsp;booktitle={Computer Vision and Pattern Recognition (CVPR)},<br>
  &nbsp;&nbsp;year={2019}<br>
  &nbsp;&nbsp;}
              </div>
              </div>
      	</div>
  	</li>
	<br>

    <li class="media">
        <a class="pull-left" href="https://carolineec.github.io/everybody_dance_now/">
          <img class="media-object img-responsive" src="images/Everybody-Dance-Now.gif" alt="motion retargeting for dance" width="200">
        </a>
        <div class="media-body">
          <h4 class="media-heading">Everybody Dance Now!
          <p><small>
        <em>
          "Do as I do" motion transfer.
        </em>
          <br>
          Given a source video of a person dancing we can transfer that performance to a novel (amateur) target after only a few minutes of the target subject performing standard moves. We pose this problem as a per-frame image-to-image translation with spatio-temporal smoothing. Using pose detections as an intermediate representation between source and target, we learn a mapping from pose images to a target subject's appearance. We adapt this setup for temporally coherent video generation including realistic face synthesis.
          </p></small>
          </h4>
                  <p>
<!--      Caroline Chan<strong> Shiry Ginosar</strong> Tinghui Zhou and Alexei A. Efros <span class="cite-title">Everybody Dance Now</span>, Computer Vision for Fashion, Art and Design Workshop, ECCV 2018. <strong>Best Paper Award</strong>. <a href="https://arxiv.org/abs/1808.07371">PDF</a>, <a href="#dance" role="button" data-toggle="collapse" data-target="#collapse-dance" aria-expanded="false" aria-controls="collapse">BibTeX</a>, <a href="https://www.youtube.com/watch?v=PCBTZh41Ris">Video</a>, <a href="https://carolineec.github.io/everybody_dance_now/">Project Page</a> -->
      Caroline Chan, <strong>Shiry Ginosar</strong>, Tinghui Zhou and Alexei A. Efros. <span class="cite-title">Everybody Dance Now</span>, ICCV 2019. <a href="https://arxiv.org/abs/1808.07371">PDF</a>,<!--  <a href="#dance" role="button" data-toggle="collapse" data-target="#collapse-dance" aria-expanded="false" aria-controls="collapse">BibTeX</a>, --> <a href="https://www.youtube.com/watch?v=PCBTZh41Ris">Video</a>, <a href="https://carolineec.github.io/everybody_dance_now/">Project Page</a>, Check out the <a href="https://apps.apple.com/us/app/sway-magic-dance/id1441466995">Sway: Magic Dance App!</a>
              <div class="collapse" id="collapse-dance">
              <div class="well">
  @inproceedings{Chan2019dance,<br>
  &nbsp;&nbsp;author = {Chan, Caroline and Ginosar, Shiry and Zhou, Tinghui and Efros, Alexei A.},<br>
  &nbsp;&nbsp;title = {Everybody Dance Now},<br>
  &nbsp;&nbsp;booktitle = {IEEE International Conference on Computer Vision (ICCV)},<br>
  &nbsp;&nbsp;year = 2019,<br>
  &nbsp;&nbsp;}
              </div>
              </div>
            </p>
        </div>
      </li>
    <br>
    <li class="media" id='art'>
        <a class="pull-left" href="publications/ginosar_computer_art.pdf">
          <img class="media-object img-responsive" src="images/art.png" alt="object detections in breughel paintings" width="200">
        </a>
        <div class="media-body">
          <h4 class="media-heading">The Burgeoning Computer-Art Symbiosis
          <p><small>
          <em>
            "Computers help us understand art. Art helps us teach computers."
          </em>
          </p></small>
          </h4>
      <p>
      <strong>Shiry Ginosar</strong>, Xi Shen, Karan Dwivedi, Elizabeth Honig, and Mathieu Aubry. <span class="cite-title">The Burgeoning Computer-Art Symbiosis</span>, XRDS: Crossroads, The ACM Magazine for Students - Computers and Art archive
Volume 24 Issue 3, Spring 2018, Pages 30-33. <a href="publications/ginosar_computer_art.pdf">PDF</a><!--  <a href="#art" role="button" data-toggle="collapse" data-target="#collapse-art" aria-expanded="false" aria-controls="collapse">BibTeX</a> -->
              <div class="collapse" id="collapse-art">
              <div class="well">
        @article{ginosar2018art,<br>
        &nbsp;&nbsp;author = {Ginosar, Shiry and Shen, Xi and Dwivedi, Karan and Honig, Elizabeth and Aubry, Mathieu},<br>
         &nbsp;&nbsp;title = {The Burgeoning Computer-art Symbiosis},<br>
         &nbsp;&nbsp;journal = {XRDS},<br>
         &nbsp;&nbsp;issue_date = {Spring 2018},<br>
         &nbsp;&nbsp;volume = {24},<br>
         &nbsp;&nbsp;number = {3},<br>
         &nbsp;&nbsp;month = apr,<br>
         &nbsp;&nbsp;year = {2018},<br>
         &nbsp;&nbsp;issn = {1528-4972},<br>
         &nbsp;&nbsp;pages = {30--33},<br>
         &nbsp;&nbsp;numpages = {4},<br>
         &nbsp;&nbsp;url = {http://doi.acm.org/10.1145/3186655},<br>
         &nbsp;&nbsp;doi = {10.1145/3186655},<br>
         &nbsp;&nbsp;acmid = {3186655},<br>
         &nbsp;&nbsp;publisher = {ACM},<br>
         &nbsp;&nbsp;address = {New York, NY, USA},<br>
        } 
              </div>
              </div>
            </p>
        </div>
      </li>
<br> <!-- add a break here !-->
    <li class="media" id="yearbooks">
        <a class="pull-left" href="projects/yearbooks/yearbooks.html">
            <img class="media-object img-responsive" src="images/what_makes_decade_labelled.png" alt="hair fashions per decade" width="200">
        </a>
        <div class="media-body">
            <h4 class="media-heading">A Century of Portraits
            <p><small>
      <em>
        "What makes the 60's look like the 60's?"
      </em>
        <br>
        Many details about our world are not captured in written records because they are too mundane or too abstract to describe in words. Fortunately, since the invention of the camera, an ever-increasing number of photographs capture much of this otherwise lost information. This plethora of artifacts documenting our “visual culture” is a treasure trove of knowledge as yet untapped by historians. We present a dataset of 37,921 frontal-facing American high school yearbook photos that allow us to use computation to glimpse into the historical visual record too voluminous to be evaluated manually. The collected portraits provide a constant visual frame of reference with varying content. We can therefore use them to consider issues such as a decade’s defining style elements, or trends in fashion and social norms over time.
            </p></small>
            </h4>
            <p>
			<strong>Shiry Ginosar</strong>, Kate Rakelly, Sarah Sachs, Brian Yin, Crystal Lee, Philipp Kr&auml;henb&uuml;hl and Alexei A. Efros. <span class="cite-title">A Century of Portraits: A Visual Historical Record of American High School Yearbooks</span>, Extreme Imaging Workshop, ICCV 2015. <strong>and</strong> IEEE Transactions on Computational Imaging, September 2017. <a href="publications/IEEE_yearbooks.pdf">PDF</a>,<!-- <a href="#yearbooks" role="button" data-toggle="collapse" data-target="#collapse-yearbooks" aria-expanded="false" aria-controls="collapse">BibTeX</a>, --> <a href="projects/yearbooks/yearbooks.html">Project Page</a>
              <div class="collapse" id="collapse-yearbooks">
              <!--<div class="well">
                @article{ginosar2015century,<br>
                &nbsp;&nbsp;title={A Century of Portraits: A Visual Historical Record of American High School Yearbooks},<br>
                &nbsp;&nbsp;author={Ginosar, Shiry and Rakelly, Kate and Sachs, Sarah and Yin, Brian and Efros, Alexei A},<br>
                &nbsp;&nbsp;journal={arXiv preprint arXiv:1511.02575},<br>
                &nbsp;&nbsp;year={2015}<br>
                }
			</div>-->
              <div class="well">
				@ARTICLE{ginosar2017yearbooks,<br> 
				&nbsp;&nbsp;author={Ginosar, Shiry and Rakelly, Kate and Sachs, Sarah M. and Yin, Brian and Lee, Crystal and Krähenbühl, Philipp and Efros, Alexei A.},<br> 
				&nbsp;&nbsp;journal={IEEE Transactions on Computational Imaging},<br>
				&nbsp;&nbsp;title={A Century of Portraits: A Visual Historical Record of American High School Yearbooks},<br> 
				&nbsp;&nbsp;year={2017},<br> 
				&nbsp;&nbsp;volume={3},<br> 
				&nbsp;&nbsp;number={3},<br> 
				&nbsp;&nbsp;pages={421-431},<br> 
				&nbsp;&nbsp;keywords={Data mining;Face;Imaging;Market research;Sociology;Statistics;Visualization;Data mining;deep learning;historical data;image dating},<br> 
				&nbsp;&nbsp;doi={10.1109/TCI.2017.2699865},<br> 
				&nbsp;&nbsp;month={Sept}<br>
				}
              </div>
              </div>
            </p>
        </div>
    </li>
	<br>

    <li class="media" id="picasso">
        <a class="pull-left" href="publications/Picasso_ECCV_2014.pdf">
            <img class="media-object img-responsive" src="images/picasso.png" alt="object detection in a Picasso image" width="200">
        </a>
        <div class="media-body">
            <h4 class="media-heading">Object Detection in Abstract Art
            <p><small>
        The human visual system is just as good at recognizing objects in paintings and other abstract depictions as it is recognizing objects in their natural form. Computer vision methods can also recognize objects outside of natural images, however their model of the visual world may not always align with the human one. If the goal of computer vision is to mimic the human visual system, then we must strive to align detection models with the human one. We propose to use Picasso's Cubist paintings to test whether detection methods mimic the human invariance to object fragmentation and part re-organization. We find that while humans significantly outperform current methods, human perception and part-based object models exhibit a similarly graceful degradation as abstraction increases, further corroborating the theory of part-based object representation in the brain.
              </p></small>

            </h4>
            <p>
              <strong>Shiry Ginosar</strong>, Daniel Haas, Timothy Brown, and Jitendra Malik. <span class="cite-title">Detecting People in Cubist Art</span>, Visart Workshop on Computer Vision for Art Analysis, ECCV 2014. <a href="publications/Picasso_ECCV_2014.pdf">PDF</a><!--  <a href="#picasso" role="button" data-toggle="collapse" data-target="#collapse-picasso" aria-expanded="false" aria-controls="collapse">BibTeX</a> -->
              <div class="collapse" id="collapse-picasso">
              <div class="well">
                @incollection{ginosar2014detecting,<br>
                &nbsp;&nbsp;title={Detecting people in Cubist art},<br>
                &nbsp;&nbsp;author={Ginosar, Shiry and Haas, Daniel and Brown, Timothy and Malik, Jitendra},<br>
                &nbsp;&nbsp;booktitle={Computer Vision-ECCV 2014 Workshops},<br>
                &nbsp;&nbsp;pages={101--116},<br>
                &nbsp;&nbsp;year={2014},<br>
                &nbsp;&nbsp;publisher={Springer International Publishing}<br>
               }
              </div>
              </div>
            </p>
        </div>
      </li>

	<br>
      <li class="media">
        <a class="pull-left" href="publications/ginosar_hearst_CHI14.pdf">
            <img class="media-object img-responsive" src="images/audio.png" alt="speech interface for document coding" width="200">
        </a>
        <div class="media-body">
            <h4 class="media-heading">Using Speech Recognition in Information Intensive Tasks
              <p><small>
                 Speech input is growing in importance, especially in mobile applications, but less research has been done on speech input for information intensive tasks like document editing and coding. This paper presents results of a study on the use of a modern publicly available speech recognition system on document coding. 
              </p></small>
            </h4>
            <p>
              <strong>Shiry Ginosar</strong>, and Marti A. Hearst. <span class="cite-title">A Study of the Use of Current Speech Recognition in an Information Intensive Task</span>, Workshop on Designing Speech and Language Interactions, CHI 2014. <a href="publications/ginosar_hearst_CHI14.pdf">PDF</a>
            </p>
        </div>
      </li>
		<br>
  		<li class="media" id="tutorials">
    		<a class="pull-left" href="publications/codeExamples_final.pdf">
      			<img class="media-object img-responsive" src="images/codeExamples.png" alt="multi-stage code examples editor" width="200">
    		</a>
    		<div class="media-body">
      			<h4 class="media-heading">Editable Code Histories
      				<p><small>
      					An IDE extension that helps with the task of authoring multi-stage code examples by allowing the author to propagate changes (insertions, deletions and modifications) throughout multiple saved stages of their code.
      				</p></small>
      			</h4>
      			<p>
      				<strong>Shiry Ginosar</strong>, Luis Fernando De Pombo, Maneesh Agrawala, and Bjoern Hartmann. <span class="cite-title">Authoring Multi-Stage Code Examples with Editable Code Histories</span>, UIST 2013. <a href="publications/codeExamples_final.pdf">PDF</a>,<!--  <a href="#tutorials" role="button" data-toggle="collapse" data-target="#collapse-tutorials" aria-expanded="false" aria-controls="collapse">BibTeX</a>, --> <a href="http://www.youtube.com/watch?v=ck-gefQfwkY&feature=player_embedded">Video</a>
              <div class="collapse" id="collapse-tutorials">
              <div class="well">
               @inproceedings{ginosar2013authoring,<br>
                &nbsp;&nbsp;title={Authoring multi-stage code examples with editable code histories},<br>
                &nbsp;&nbsp;author={Ginosar, Shiry and Pombo, De and Fernando, Luis and Agrawala, Maneesh and Hartmann, Bjorn},<br>
                &nbsp;&nbsp;booktitle={Proceedings of the 26th annual ACM symposium on User interface software and technology},<br>
                &nbsp;&nbsp;pages={485--494},<br>
                &nbsp;&nbsp;year={2013},<br>
                &nbsp;&nbsp;organization={ACM}<br>
              }
              </div>
              </div>
      			</p>
    		</div>
  		</li>

	<br>
      <li class="media" id="crowdsourcing">
        <a class="pull-left" href="#publications/CrowdAnalytics-VAST2013-FinalCameraReady.pdf">
          <img class="media-object" src="images/wes.png" alt="crowdsourced data analysis workflow" width="200">
        </a>
        <div class="media-body">
          <h4 class="media-heading">Crowdsourced Data Analysis
            <p><small>
              A system that lets analysts use paid crowd workers to explore data sets and helps analysts interactively examine and build upon workers' insights.
            </p></small>
          </h4>
          <p>
            Wesley Willett, <strong>Shiry Ginosar</strong>, Avital Steinitz, Bjoern Hartmann, and Maneesh Agrawala. <span class="cite-title">Identifying Redundancy and Exposing Provenance in Crowdsourced Data Analysis</span>, IEEE Transactions on Visualization and Computer Graphics, 2013. <a href="publications/CrowdAnalytics-VAST2013-FinalCameraReady.pdf">PDF</a><!--  <a href="#crowdsourcing" role="button" data-toggle="collapse" data-target="#collapse-crowdsourcing" aria-expanded="false" aria-controls="collapse">BibTeX</a> -->
              <div class="collapse" id="collapse-crowdsourcing">
              <div class="well">
              @article{willett2013identifying,<br>
                &nbsp;&nbsp;title={Identifying Redundancy and Exposing Provenance in Crowdsourced Data Analysis},<br>
                &nbsp;&nbsp;author={Willett, Wesley and Ginosar, Shiry and Steinitz, Avital and Hartmann, Bjorn and Agrawala, Maneesh},<br>
                &nbsp;&nbsp;journal={Visualization and Computer Graphics, IEEE Transactions on},<br>
                &nbsp;&nbsp;volume={19},<br>
                &nbsp;&nbsp;number={12},<br>
                &nbsp;&nbsp;pages={2198--2206},<br>
                &nbsp;&nbsp;year={2013},<br>
                &nbsp;&nbsp;publisher={IEEE}<br>
              }
              </div>
              </div>
          </p>
        </div>
      </li>
      <br>

  		<li class="media" id="phetch">
    		<a class="pull-left" href="#publications/phetch-chi-2006.pdf">
      			<img class="media-object img-responsive" src="images/phetch.png" alt="phetch game logo" width="200">
    		</a>
    		<div class="media-body">
      			<h4 class="media-heading">
      				Phetch - A Human Computation Game
      				<p><small>
      					<a href="http://en.wikipedia.org/wiki/Phetch">Phetch</a> is an online game which collects natural language descriptions for images on the web as a side effect of game play. Can be used to improve the accessibility of the web as well as improve upon current image search engines.
      				</small></p>
      			</h4>
      			<p>	
    				<strong>Shiry Ginosar</strong>, <span class="cite-title">Human Computation for HCIR Evaluation</span>, Proceedings, HCIR 2007, pp. 40-42. <a href="publications/hcir07-1.pdf">PDF</a>
    			</p>
    			<p>
    				Luis von Ahn, <strong>Shiry Ginosar</strong>, Mihir Kedia, and Manuel Blum. <span class="cite-title">Improving Image Search with Phetch</span>, ICASSP 2007. <a href="publications/Phetch_ICASSP.pdf">PDF</a>,<!--  <a href="#phetch" role="button" data-toggle="collapse" data-target="#collapse-image-search" aria-expanded="false" aria-controls="collapse">BibTeX</a> -->
              <div class="collapse" id="collapse-image-search">
              <div class="well">
              @inproceedings{von2007improving,<br>
                &nbsp;&nbsp;title={Improving image search with phetch},<br>
                &nbsp;&nbsp;author={Von Ahn, Luis and Ginosar, Shiry and Kedia, Mihir and Blum, Manuel},<br>
                &nbsp;&nbsp;booktitle={Acoustics, speech and signal processing, 2007. icassp 2007. ieee international conference on},<br>
                &nbsp;&nbsp;volume={4},<br>
                &nbsp;&nbsp;pages={IV--1209},<br>
                &nbsp;&nbsp;year={2007},<br>
                &nbsp;&nbsp;organization={IEEE}<br>
              }
              </div>
              </div>
    			</p>
    			<p>
    				Luis von Ahn, <strong>Shiry Ginosar</strong>, Mihir Kedia, Ruoran Liu, and Manuel Blum. <span class="cite-title">Improving Accessibility of the Web with a Computer Game</span>, CHI 2006. <strong>Honorable mentioned paper and nominee for Best of CHI award</strong>. <a href="publications/phetch-chi-2006.pdf">PDF</a>,<!--  <a href="#phetch" role="button" data-toggle="collapse" data-target="#collapse-phetch" aria-expanded="false" aria-controls="collapse">BibTeX</a>, --> <a href="http://www.newscientist.com/article/dn9177-gamers-help-the-blind-get-the-picture.html">Press Coverage</a>
              <div class="collapse" id="collapse-phetch">
              <div class="well">
              @inproceedings{von2006improving,<br>
                &nbsp;&nbsp;title={Improving accessibility of the web with a computer game},<br>
                &nbsp;&nbsp;author={Von Ahn, Luis and Ginosar, Shiry and Kedia, Mihir and Liu, Ruoran and Blum, Manuel},<br>
                &nbsp;&nbsp;booktitle={Proceedings of the SIGCHI conference on Human Factors in computing systems},<br>
                &nbsp;&nbsp;pages={79--82},<br>
                &nbsp;&nbsp;year={2006},<br>
                &nbsp;&nbsp;organization={ACM}<br>
              }
              </div>
              </div>
				  </p>
    		</div>
  		</li>

		</ul>

<h2>Other Projects</h2>

<ul class="media-list, citations">

  <li class="media">
    <a class="pull-left" href="#">
      <img class="media-object" src="images/H20-IQ.jpg" alt="H20-IQ device" width="200">
    </a>
    <div class="media-body">
      <h4 class="media-heading">H20-IQ
        <p><small>
          A tablet-controlled, solar-powered drip irrigation system. A humidity sensor at the tip of each "spike" records soil moisture; an internal servo in the 3D-printed enclosure opens and closes a drip irrigation line valve. Individual devices in a garden communicate with a central garden server, which also acts as a webserver that hosts the HTML-based user interface. Gardeners can review graphs of humidity readings over time and adjust waterning plans through this Web application.
        </p></small>
      </h4>
        <p>
          Joint class project with Valkyrie Savage and Mark Fuge.
        </p>
        <p>
          Featured in Bjoern Hartmann and Paul K. Wright <span class="cite-title">Designing Bespoke Interactive Devices</span>, IEEE Computer August 2013, Volume 46, Number 8. <a href="http://issuu.com/min-mag/docs/ieee_computer_aug_2013/86">Article</a>
        </p>
    </div>
  </li>

</ul>

<h2>Teaching</h2>
<strong>Co-Teacher and GSI</strong>, Image Manipulation and Computational Photography, <a href="http://inst.eecs.berkeley.edu/~cs194-26/fa18/">Fall 2018</a>
<br>
<strong>GSI</strong>, Image Manipulation and Computational Photography, <a href="http://inst.eecs.berkeley.edu/~cs194-26/fa14/">Fall 2014</a>

<br>
<h2>Undergraduate and MA Researchers</h2>
<p>
  I am actively looking for exceptional undergraduate students in their third year of the U Chicago CS program who have excelled in the Machine Learning and/or Computer Vision classes. If you are interested in a research position and think you are qualified, do send me a note!
</p>
<dl class="dl-horizontal">
	<dt>Former Students</dt>
	<dd><a href="https://www.linkedin.com/in/vivienn">Vivien Nguyen</a> (Now @ Princeton)</dd>
 	<dd><a href="https://www.linkedin.com/in/varsha-r">Varsha Ramakrishnan</a></dd>
    <dd><a href="https://www.linkedin.com/in/gefenkohavi">Gefen Kohavi</a></dd>
    <dd>Caroline Mai Chan (Now @ MIT)</dd>
    <dd><a href="https://www.linkedin.com/in/hemangjangle/">Hemang Jeetendra Jangle</a></dd>
    <dd><a href="https://www.linkedin.com/in/daniel-tsai-76719b55">Daniel Tsai</a></dd>
    <dd><a href="https://www.linkedin.com/in/crystal-lee-678a7758">Crystal Lee</a></dd>
    <dd><a href="http://people.eecs.berkeley.edu/~rakelly/">Kate Rakelly</a> (Now @ UC Berkeley)</dd>
    <dd><a href="http://www.linkedin.com/pub/brian-yin/86/143/b83">Brian Yin</a></dd>
		<dd><a href="http://www.sarahmsachs.com">Sarah Sachs</a></dd>
		<dd><a href="https://www.linkedin.com/in/timbrownatlinkedin">Timothy Brown</a></dd>
		<dd><a href="http://www.linkedin.com/in/lfdepombo">Luis Fernando de Pombo</a></dd>
</dl>


	</div> <!-- middle column -->

<div class="col-md-2">
	<!-- empty room saver on right -->
</div>

</div>
<footer>
  <br/>
  <br/>
</footer>
</div>
</body>
</html>
