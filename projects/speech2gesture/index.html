<!DOCTYPE html>
<html lang="en">
<head>
<br/>
<br/>
<title>Speech2Gesture</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0" charset="utf-8">
<!-- jQuery -->
<script src="http://code.jquery.com/jquery.min.js"></script>
<!-- Bootstrap -->
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
<!-- Bootstrap -->
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>
<link rel="stylesheet" type="text/css" href="http://people.eecs.berkeley.edu/~shiry/styles/main.css"/>
</head>
<body>

<div class="container">
<div class="row">
	<div class="col-md-12">
		<div class="page-header">
			<h1 class="text-center">Learning Individual Styles of Conversational Gesture</h1>
		</div>
	</div>
</div>
<br>
<div class="row"> <!-- names row-->
		<div class="col-md-2">
			<h5 class="text-center"><a href="http://people.eecs.berkeley.edu/~shiry">Shiry Ginosar*</a></h5>
		</div>
		<div class="col-md-2">
			<h5 class="text-center"><a href="https://amirbar.github.io/">Amir Bar*</a></h5>
		</div>
		<div class="col-md-2">
			<h5 class="text-center">Gefen Kohavi</h5>
		</div>
		<div class="col-md-2">
			<h5 class="text-center"><a href="https://www.csail.mit.edu/person/caroline-chan">Caroline Chan</a></h5>
		</div>
		<div class="col-md-2">
			<h5 class="text-center"><a href="http://andrewowens.com/">Andrew Owens</a></h5>
		</div>
		<div class="col-md-2">
			<h5 class="text-center"><a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a></h5>
		</div>
</div>
<br>
<div class="row">
	<div class="col-md-3">
	</div>
	<div class="col-md-2">
		<h5 class="text-right">UC Berkeley</h5>
	</div>
	<div class="col-md-2">
		<h5 class="text-center">Zebra Medical</h5>
	</div>
	<div class="col-md-2">
		<h5 class="text-left">MIT</h5>
	</div>
	<div class="col-md-3">
	</div>
</div>

<br>
<div class="row">
	<div class="col-md-12">
		<h5 class="text-center">CVPR 2019</h5>
	</div>
<div>
<br>
<div class="row">
	<div class="col-md-5">
	</div>
	<div class="col-md-1">
		<h3 class="text-center"><a href="https://github.com/amirbar/speech2gesture">[Code]</a></h3>
	</div>
	<div class="col-md-1">
		<h3 class="text-center"><a href="https://drive.google.com/drive/folders/1qvvnfGwas8DUBrwD4DoBnvj8anjSLldZ?usp=sharing">[Data]</a></h3>
	</div>
	<div class="col-md-5">
	</div>
<div>

<div class="row">
	<div class="col-md-1">
		<!--left margin column-->
	</div>

	<div class="col-md-10 text-center"> <!--main content column-->

	<p>
	    <figure class="figure">
	      <img src="images/teaser_gan_oliver_041.png" class="img-fluid mx-auto" alt="XXX">
	      <figcaption class="figure-caption">Speech audio-to-gesture translation. From the bottom upward: the input audio, predicted arm and hand motion, and synthesized video frames.</figcaption>
	    </figure>
	</p>

	<br>
    <h2>Abstract</h2>
     <br>
    <p class="text-justify">
    Human speech is often accompanied by hand and arm gestures.
Given audio speech input, we generate plausible gestures to go along with the sound.
Specifically, we perform cross-modal translation from "in-the-wild" monologue speech of a
single speaker to their hand and arm motion. 
We train on unlabeled videos for which we only have noisy pseudo ground truth from an automatic pose detection system.
Our proposed model significantly outperforms baseline methods in a quantitative comparison.
To support research toward obtaining a computational understanding of the relationship between gesture and speech, we release a large video dataset of person-specific gestures.
    </p>

	<br>
 	<hr>
 	<br>

 <h2>Paper</h2>
  <br>
    <ul class="media-list, citations">
    <li class="media" id="gestures">
        <a class="pull-left" href="https://arxiv.org/abs/1906.04160">
            <img class="media-object img-fluid img-thumbnail mr-4" src="images/gestures_paper.png" alt="gestures paper" width="150">
        </a>
        <div class="media-body">
            <h5 class="media-heading text-left">Learning Individual Styles of Conversational Gesture</h5>
            <p class="text-left">
              Shiry Ginosar*, Amir Bar*, Gefen Kohavi, Caroline Chan, Andrew Owens and Jitendra Malik
              <span class="cite-title">Learning Individual Styles of Conversational Gesture</span>,
              Computer Vision and Pattern Recognition, CVPR 2019.
              <br>
              <a href="https://arxiv.org/abs/1906.04160">PDF</a>, <a href="#gestures" role="button" data-toggle="collapse" data-target="#collapse-gestures" aria-expanded="false" aria-controls="collapse">BibTeX</a>
              <div class="collapse" id="collapse-gestures">
              <div class="card text-left bg-light mb-4">
                @InProceedings{ginosar2019gestures,<br> 
        &nbsp;&nbsp;author={S. Ginosar and A. Bar and G. Kohavi and C. Chan and A. Owens and J. Malik},<br> 
        &nbsp;&nbsp;title = {Learning Individual Styles of Conversational Gesture},<br>
        &nbsp;&nbsp;booktitle = {Computer Vision and Pattern Recognition (CVPR)}<br> 
        &nbsp;&nbsp;publisher = {IEEE},<br>
        &nbsp;&nbsp;year={2019},<br> 
        &nbsp;&nbsp;month=jun<br>
        }
              </div>
              </div>
            </p>
        </div>
    </li>
  </ul>

    <br>
 	<hr>
 	<br>

 	<h2>Demo</h2>
 	<br>
 	<iframe width="560" height="315" src="https://www.youtube.com/embed/xzTE5sobpFY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

  	<br>
 	<hr>
 	<br>

	<h2>Code</h2>
	<p>
	<h3 class="text-center"><a href="https://github.com/amirbar/speech2gesture">[Tensorflow]</a></h3>
	</p>

 	<br>
 	<hr>
 	<br>

   	 <h2>Data</h2>
   	 <br>
   	 <div>
   	    <img src="images/ellen_overlay.gif" alt="Ellen Gestures" width=175>
		<img src="images/conan_overlay.gif" alt="Conan Gestures" width=175>
		<img src="images/seth_overlay.gif" alt="Seth Gestures" width=175>
		<img src="images/chemistry_overlay.gif" alt="Chemistry Gestures" width=175>
		<img src="images/oliver_overlay.gif" alt="Oliver Gestures" width=175>
 	</div>
	<br>
   	 <p class="text-justify">
   	 	We present a large, 144-hour person-specific video dataset of 10 speakers, with frame-by-frame automatically-detected pose annotations. We deliberately pick a set of speakers for which we can find hours of clean single-speaker footage. Our speakers come from a diverse set of backgrounds: television show hosts, university lecturers and televangelists. They span at least three religions and discuss a large range of topics from commentary on current affairs through the philosophy of death, chemistry and the history of rock music, to readings in the Bible and the Qur'an.
   	 </p>
   	<div>
   		<a href="https://drive.google.com/drive/folders/1qvvnfGwas8DUBrwD4DoBnvj8anjSLldZ?usp=sharing" class="btn btn-outline-secondary">Download</a>
   	</div>

   	<br>
 	<hr>
 	<br>

   	 <h2>Press Coverage</h2>
   	 <br>
   	 <div class="row">
    	<div class="col-md-12">
      		<a href="https://www.sciencemag.org/news/2019/06/watch-artificial-intelligence-predict-conan-o-brien-s-gestures-just-sound-his-voice">
        	<img class="img-responsive" src="images/press/science.jpg" alt="sciencemag logo" width="100">
      		</a>
    	</div>
    </div>
   	 

    <br>
 	<hr>
 	<br>

 	 <h2>Acknowledgements</h2>
 	  <br>
 	 <p class="text-justify">
 	 	This work was supported, in part, by the AWS Cloud Credits for Research and the DARPA MediFor programs, and the UC Berkeley Center for Long-Term Cybersecurity. Special thanks to Alyosha Efros, the bestest advisor, and to Tinghui Zhou for his dreams of late-night talk show stardom.
 	 </p>

	</div> <!-- close middle column -->
	<div class="col-md-1">
	<!-- empty room saver on right -->
	</div>
</div> <!-- close main row -->
</div> <!-- close body container --> 
<footer>
  <br/>
  <br/>
</footer>
</div>
</body>
</html>
